{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda3/envs/ML/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/kevin/anaconda3/envs/ML/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import cross_validation\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn import svm\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSetDir = \"/home/kevin/LOTTERHOS_LAB/TTT_RecombinationGenomeScans/ml_project/feature_vecs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using these features: all (indices: [1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "classifierPickleFileName = \"clfAcrossGenomeSMOTE1000.p\"\n",
    "statsToUse = \"all\"\n",
    "classList = []\n",
    "trainingData = []\n",
    "labelToClassName = {}\n",
    "headerH = {}\n",
    "\n",
    "for trainingSetFileName in os.listdir(trainingSetDir):\n",
    "    classList.append(trainingSetFileName.split(\".fvec\")[0])\n",
    "    trainingSetFile = open(trainingSetDir + \"/\" + trainingSetFileName)\n",
    "    currTrainingData = trainingSetFile.readlines()\n",
    "    trainingSetFile.close()\n",
    "\n",
    "    trainingData += currTrainingData[1:]#append all training data from the current set (minus the header)\n",
    "\n",
    "    currLabelH = {}\n",
    "    for example in currTrainingData[1:]:\n",
    "        currLabelH[example.split(\"\\t\")[0]] = 1\n",
    "    assert len(currLabelH) == 1\n",
    "    labelToClassName[currLabelH.keys()[0]] = trainingSetFileName.split(\".fvec\")[0]\n",
    "    \n",
    "    header = currTrainingData[0].strip().split(\"\\t\")\n",
    "    headerH[currTrainingData[0].strip()] = 1\n",
    "    assert header[0] == \"classLabel\"\n",
    "    statIndices = []\n",
    "    if \"all\" in statsToUse:\n",
    "        statIndices = range(1, len(header))\n",
    "    else:\n",
    "        for i in range(1, len(header)):\n",
    "            if header[i] in statsToUse or header[i].split(\"_win\")[0] in statsToUse:\n",
    "                statIndices.append(i)\n",
    "assert len(headerH) == 1\n",
    "\n",
    "sys.stderr.write(\"using these features: %s (indices: %s)\\n\" %(str(statsToUse), str(statIndices)))\n",
    "XH = {}\n",
    "for i in range(len(trainingData)):\n",
    "    trainingData[i] = trainingData[i].strip().split(\"\\t\")\n",
    "    currVector = []\n",
    "    if not \"nan\" in trainingData[i]:\n",
    "        for j in statIndices:\n",
    "            currVector.append(float(trainingData[i][j]))\n",
    "        assert len(currVector) == len(statIndices)\n",
    "        if not XH.has_key(trainingData[i][0]):\n",
    "            XH[trainingData[i][0]] = []\n",
    "        XH[trainingData[i][0]].append(currVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training set size after split: 4079\n",
      "Testing set size: 1360\n",
      "training set size after balancing: 5257\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "#balance the training set\n",
    "# minClassSize = min([len(XH[classLabel]) for classLabel in  XH.keys()])\n",
    "X = []\n",
    "y = []\n",
    "for classLabel in sorted(XH.keys()):\n",
    "    random.shuffle(XH[classLabel])\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            currVector = XH[classLabel][i]\n",
    "        except IndexError:\n",
    "            break\n",
    "        X.append(currVector)\n",
    "        y.append(classLabel)\n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "sys.stderr.write(\"Training set size after split: %s\\n\" %(len(y_train)))\n",
    "sys.stderr.write(\"Testing set size: %s\\n\" %len(y_test))\n",
    "\n",
    "# SMOTE:\n",
    "X_resampled, y_resampled = SMOTE().fit_sample(X_train, y_train)\n",
    "sys.stderr.write(\"training set size after balancing: %s\\n\" %(len(y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT=neut_R=invers\n",
      "MT=neut_R=BS\n",
      "MT=sweep_R=neutral\n",
      "MT=neut_R=lowRC\n",
      "MT=QTN_R=neutral\n",
      "MT=delet_R=BS\n",
      "MT=neut_R=neutral\n"
     ]
    }
   ],
   "source": [
    "for classLabel in XH.keys():\n",
    "    print(classLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking accuracy when distinguishing among all 7 classes\n",
      "Training extraTreesClassifier\n",
      "GridSearchCV took 662.44 seconds for 432 candidate parameter settings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for extraTreesClassifier\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.751 (std: 0.019)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': 3, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.748 (std: 0.015)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 3, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.747 (std: 0.020)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': 9, 'max_depth': None}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['clfAcrossGenomeSMOTE1000.p']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.stderr.write(\"Checking accuracy when distinguishing among all %s classes\\n\" %(len(XH.keys())))\n",
    "\n",
    "maxMaxFeatures = len(X[0])\n",
    "param_grid_forest = {\"max_depth\": [3, 10, None],\n",
    "              \"max_features\": [1, 3, int(maxMaxFeatures**0.5), maxMaxFeatures],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "clf, mlType, paramGrid = ExtraTreesClassifier(n_estimators=100), \"extraTreesClassifier\", param_grid_forest\n",
    "\n",
    "heatmap = []\n",
    "sys.stderr.write(\"Training %s\\n\" %(mlType))\n",
    "grid_search = GridSearchCV(clf,param_grid=param_grid_forest,cv=10,n_jobs=-1)\n",
    "start = time()\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "sys.stderr.write(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\\n\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "print \"Results for %s\" %(mlType)\n",
    "report(grid_search.grid_scores_)\n",
    "joblib.dump((X_test, y_test, grid_search), classifierPickleFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
