{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import re, subprocess, os.path, time, argparse, pkg_resources, sys\n",
    "import numpy as np, pandas as pd\n",
    "import allel\n",
    "from allel.model.ndarray import SortedIndex\n",
    "from allel.util import asarray_ndim\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import warnings\n",
    "# filter out the future warning from standardize_by_allele_count\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#############################################################################################\n",
    "# File    : calcAdditionalStatsFromVCF.py\n",
    "# History : 10-17-2018 - Created by Kevin Freeman (KF)\n",
    "#           11-19-2018 - updated to use multiprocessing and break windows at chroms\n",
    "#--------------------------------------------------------------------------------------------\n",
    "# This is a script that uses scikit-allel to calculate several summary statistics on a set\n",
    "# of simulations, stored in VCFs. The only required argument is --dataDir [path to vcf files]\n",
    "# but there are 2 optional args: --ncpus and --winodow. Run with the -help option to\n",
    "# see full usage information.\n",
    "#\n",
    "# Stats the scripts calculates (updated 11-19):\n",
    "#  - Tajima's D\n",
    "#  - pi (sequence diversity)\n",
    "#  - watterson's theta\n",
    "#  - Garud's H12\n",
    "#  - Garud's H2/H1\n",
    "#  - IHS (integrated haplotype score)\n",
    "#\n",
    "###############################################################################################\n",
    "\n",
    "### small function to print to stderr instead of stdout\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------\n",
    "# calcAndAppendStatValForScan:\n",
    "#\n",
    "# The calculations for each statistic. Takes the name of the statistic and the relevant \n",
    "# data structures and performs the right calculations. Appends the result to statVals\n",
    "# ----------------------------------------------------------------------------------------\n",
    "#\n",
    "# snpLocs : a numpy array with the positions in bp for each snp in the window\n",
    "# statName: which stat to calculate \n",
    "# statVals: dictionary with stat names as the keys and calculated stats for every snp in\n",
    "#           order as the values\n",
    "# subWinStart : start of the subwindow (bp)\n",
    "# subWinEnd   : end of the subwindow (bp)\n",
    "# alleleCounts: numpy array representing the # of calls of each allele per variant \n",
    "#\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "def calcAndAppendStatValForScan(snpLocs, statName, hapsInSubWin, statVals,\n",
    "                                subWinStart = None, subWinEnd = None, alleleCounts = None,\n",
    "                               altAlleleCounts = None):\n",
    "# modified code from https://github.com/kern-lab/diploSHIC/blob/master/fvTools.py\n",
    "    if statName == \"tajD\":\n",
    "        statVals[statName].append(allel.stats.diversity.tajima_d(\n",
    "            alleleCounts, pos=snpLocs, start=subWinStart, stop=subWinEnd))\n",
    "    elif statName == \"pi\":\n",
    "        statVals[statName].append(allel.stats.diversity.sequence_diversity(\n",
    "            snpLocs, alleleCounts, start=subWinStart, stop=subWinEnd))\n",
    "    elif statName == \"thetaW\":\n",
    "        statVals[statName].append(allel.stats.diversity.watterson_theta(\n",
    "            snpLocs, alleleCounts, start=subWinStart, stop=subWinEnd)) \n",
    "    elif statName == 'H2/H1' or statName == 'H12':\n",
    "        h1,h12,h123,h21 = allel.stats.selection.garud_h(hapsInSubWin)\n",
    "        if statName == 'H2/H1':\n",
    "            statVals[statName].append(h21)\n",
    "        else:\n",
    "            statVals[statName].append(h12)\n",
    "    elif statName == 'ihs':\n",
    "        unstd_ihs = allel.ihs(hapsInSubWin, snpLocs, map_pos = None, use_threads = False)\n",
    "        statVals[statName].extend(allel.stats.selection.standardize_by_allele_count(unstd_ihs, \n",
    "                                                                                   altAlleleCounts,\n",
    "                                                                                   n_bins=20)[0])            \n",
    "    elif statName == 'nsl':\n",
    "        unstd_nsl = allel.nsl(hapsInSubWin, use_threads = False)\n",
    "        statVals[statName].extend(allel.stats.selection.standardize_by_allele_count(unstd_nsl, \n",
    "                                                                                   altAlleleCounts,\n",
    "                                                                                   n_bins=20)[0]) \n",
    "    else:\n",
    "        eprint(\"Unable to calculate \" + statName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------\n",
    "# processFile:\n",
    "# \n",
    "# Where the data processing gets done. Takes a single vcf file, divides it into chromosomes and\n",
    "# then into windows, and sends it to calcAndAppendStatValForScan, which does the actual calculations\n",
    "# It then takes the results of these calculations and prints them to a new output file\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# Inputs  : the name of a vcf file (string)\n",
    "# Outputs : a new file called \"XXXX_Invers_ScanResults_with_sk-allel.txt\", where XXXX is the sim number\n",
    "#           The file contains any stats that were previously calculated in XXXX_Invers_ScanResults.txt\n",
    "#           along with the new stats calculated in sci-kit allel\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def processFile(f):\n",
    "    eprint(\"Processing \" + f)\n",
    "    \n",
    "    vcf     = allel.read_vcf(f, fields = [\"CHROM\", \"POS\", \"GT\"])\n",
    "    m       = re.search('[0-9]{5}', f)      # m is a regex match object\n",
    "    simNum  = m.group(0)                    # sim number is the first set of 5 numbers in the vcf name\n",
    "    \n",
    "    g           = allel.GenotypeArray(vcf[\"calldata/GT\"])\n",
    "    ac          = g.count_alleles()\n",
    "    aac         = ac[:,1]                 # [:,1] is all the alternate allele counts, [:,0] is ref\n",
    "    haps        = g.to_haplotypes()\n",
    "    posSeries   = pd.Series(vcf['variants/CHROM'], index = vcf['variants/POS'])\n",
    "    # cast chroms to int so we get numeric sort instead of alphabetic sort in groupby\n",
    "    groupedPos = posSeries.groupby(by = [int(c) for c in vcf['variants/CHROM']])\n",
    "    \n",
    "    statvals = { \"tajD\"   : [],\n",
    "                 \"pi\"     : [],\n",
    "                 \"thetaW\" : [],\n",
    "                 \"H12\"    : [],\n",
    "                 \"H2/H1\"  : [],\n",
    "                 \"ihs\"    : [],\n",
    "                 \"nsl\"    : [] }\n",
    "    \n",
    "    for chrom, group in groupedPos:             # only consider 1 chrom at a time to preserve breaks\n",
    "        pos = list(group.index)\n",
    "        chromStart = np.searchsorted(vcf['variants/POS'], pos[0])\n",
    "        chromEnd   = np.searchsorted(vcf['variants/POS'], pos[-1], side = \"right\")\n",
    "        chromHap   = haps.subset(list(range(chromStart, chromEnd)))\n",
    "        \n",
    "        # now calc ihs and nsl, they can just take the whole chromosome without need for a sliding window\n",
    "        chromAac  = aac[chromStart:chromEnd]\n",
    "        if \"ihs\" in statvals.keys():\n",
    "            calcAndAppendStatValForScan(snpLocs = list(pos), statName = 'ihs', statVals = statvals,\n",
    "                                        hapsInSubWin = chromHap, altAlleleCounts = chromAac)\n",
    "        if \"nsl\" in statvals.keys():\n",
    "            calcAndAppendStatValForScan(snpLocs = list(pos), statName = 'nsl', statVals = statvals,\n",
    "                                        hapsInSubWin = chromHap, altAlleleCounts = aac[chromStart:chromEnd])\n",
    "        # calc the rest of the stats over a 1000bp sliding window\n",
    "        for SNP in pos:\n",
    "            winStart = SNP - int(winsize/2)\n",
    "            winEnd   = SNP + int(winsize/2)\n",
    "        \n",
    "            if winStart < pos[0]:\n",
    "                winStart = pos[0]\n",
    "            if winEnd > pos[-1]:\n",
    "                winEnd = pos[-1]\n",
    "        \n",
    "            ####### subset the haplotype array ##############\n",
    "            # search sorted finds the place in a sorted array where a number can be inserted\n",
    "            startInd = np.searchsorted(pos, winStart)                   \n",
    "            endInd   = np.searchsorted(pos, winEnd)\n",
    "        \n",
    "            hapsInSubWin = chromHap.subset(list(range(startInd, endInd + 1)))\n",
    "    \n",
    "            \n",
    "            for statName in [key for key in statvals.keys() if key !='ihs' and key != 'nsl']:\n",
    "                calcAndAppendStatValForScan(list(pos), statName, hapsInSubWin, statvals, winStart, winEnd, ac)\n",
    "        \n",
    "    \n",
    "    scanResultsFile = datadir + simNum + \"_Invers_ScanResults.txt\"\n",
    "    outfile         = datadir + simNum + \"_Invers_ScanResults_with_sk-allel.txt\"\n",
    "    \n",
    "    try:\n",
    "        newStatDf = pd.DataFrame.from_dict(statvals)\n",
    "        newStatDf = newStatDf.fillna(\"NA\")\n",
    "    except:\n",
    "        eprint([\"length of stat: \" + stat + \" = \" + str(len(statvals[stat])) for stat in list(statvals.keys())])\n",
    "        sys.exit()\n",
    "    # add version to stat column names\n",
    "    newStatDf.columns = [stat + \"_sk-allel_v\" + skVersion for stat in list(statvals.keys())] \n",
    "    \n",
    "    allFeatures = []\n",
    "    try:\n",
    "        with open (scanResultsFile, 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.split()\n",
    "                features = [f.strip('\"') for f in features]\n",
    "                allFeatures.append(features)\n",
    "            stats  = allFeatures[1:]\n",
    "            header = allFeatures[0]\n",
    "            featDf =  pd.DataFrame(stats, columns = header)\n",
    "        featDf = featDf.join(newStatDf)\n",
    "    except: \n",
    "        featDf = newStatDf\n",
    "        \n",
    "    featDf.to_csv(outfile, sep = \" \", index = False)\n",
    "    eprint(\"Created \" + outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: calcAdditionalStatsFromVCF.py -d path/directory -n 4 -w 1000\n",
      "ipykernel_launcher.py: error: the following arguments are required: -d/--datadir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "############## Main ############################################################\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    ######## Parse arguments ######\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description = 'Calculate summary statistics using scikit allel on the input VCFs',\n",
    "        usage       = \"calcAdditionalStatsFromVCF.py -d path/directory -n 4 -w 1000\")\n",
    "    parser.add_argument(\"-d\", \"--datadir\", required = True,\n",
    "                    help = \"full path to the directory with the VCFs\")\n",
    "    parser.add_argument(\"-n\", \"--ncpus\", default = cpu_count(), type = int,\n",
    "                    help = \"number of cpus to use (default: all of them)\")  \n",
    "    parser.add_argument(\"-w\", \"--window\", default = 1000, type = int,\n",
    "                    help = \"size (in bp) of sliding window around each SNP for calculations (default: 1000)\")\n",
    "\n",
    "    args     = vars(parser.parse_args())\n",
    "    datadir  = args['datadir'] \n",
    "    nproc    = args['ncpus']\n",
    "    winsize  = args['window']\n",
    "        \n",
    "    #### generate file list, find sk-a version ################\n",
    "    skVersion = pkg_resources.get_distribution(\"scikit-allel\").version # for labeling output\n",
    "    if os.path.isdir(datadir):\n",
    "        if not re.search('/$', datadir):\n",
    "            datadir = datadir + \"/\"\n",
    "        cmd     = 'ls ' + datadir + '*.vcf.gz'\n",
    "    elif os.path.isfile(datadir):\n",
    "        file    = datadir\n",
    "        datadir = re.sub('/.+?$', '', file)\n",
    "        cmd     = 'echo ' + file\n",
    "    else:\n",
    "        eprint(\"No file or directory found matching: \" + datadir)\n",
    "        sys.exit()\n",
    "    fileList =  subprocess.run(cmd, shell = True, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    fileList = fileList.split(\"\\n\")\n",
    "    fileList = list(filter(None, fileList))            # remove blank entries\n",
    "    \n",
    "    if (len(fileList) < 1):\n",
    "        print(fileList)\n",
    "        eprint(\"No gzipped vcf files found in directory: \" + datadir + \" aborting\")\n",
    "        sys.exit()\n",
    "    \n",
    "    ####  analyze vcfs in fileList in parallel ###########\n",
    "    with Pool(processes = nproc) as pool:             # launch parallel processes\n",
    "         pool.map(processFile, fileList)\n",
    "    secs = time.time() - start_time\n",
    "    \n",
    "    eprint(\"\\nIt took {} hrs {} minutes {} seconds to process {} simulation files\".format(secs // 360, \n",
    "                                                                                   secs % 360 // 60,\n",
    "                                                                                   round(secs % 360 % 60, 2), \n",
    "                                                                                   len(fileList)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
